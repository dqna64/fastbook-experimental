{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/conda/envs/fastai/lib/python3.8/site-packages (0.18.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/fastai/lib/python3.8/site-packages (from gym) (1.5.3)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from gym) (7.2.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from gym) (1.19.4)\n",
      "Requirement already satisfied: future in /opt/conda/envs/fastai/lib/python3.8/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.7.0\n",
      "gym version: 0.18.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"gym version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, hidden_act_fn=nn.Tanh, output_act_fn=nn.Identity):\n",
    "    '''\n",
    "    sizes is list of integers specifying the number of nodes\n",
    "        in each layer of the network, including input and output layers.\n",
    "    Returns a torch.nn.Sequential object.\n",
    "    '''\n",
    "    assert isinstance(sizes, list)\n",
    "    layers = []\n",
    "    num_gaps = len(sizes) - 1\n",
    "    for i in range(num_gaps):\n",
    "        act_fn = hidden_act_fn if i < num_gaps-1 else output_act_fn\n",
    "        layers.extend([nn.Linear(sizes[i], sizes[i+1]), act_fn()])\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (3): Identity()\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = mlp([4,32,2]).to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Future Discounted Rewards helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_discount(trajec_rewards, gamma=0.99):\n",
    "    '''\n",
    "    trajec_rewards must be a list of scalar reward values for each step.\n",
    "    Returns a list of reverse-accumlated, discounted rewards, where each value\n",
    "        represents the cumulative discounted rewards from that step onwards up to the end of the trajectory.\n",
    "    '''\n",
    "    assert isinstance(trajec_rewards, list)\n",
    "    trajec_len = len(trajec_rewards)\n",
    "    cum_disc_rewards = [None for i in range(trajec_len)]\n",
    "    for step in reversed(range(trajec_len)):\n",
    "        cum_disc_rewards[step] = trajec_rewards[step] + gamma * (cum_disc_rewards[step+1] if step+1 < trajec_len else 0)\n",
    "    return cum_disc_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, -1, 4, 5, 3, 5, 2, 0, -1, 5, -2, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_rewards = np.random.randint(-2, 6, size=(12,)).tolist()\n",
    "ep_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21.03300922553339,\n",
       " 20.235362854074133,\n",
       " 21.44986146876175,\n",
       " 17.62612269571894,\n",
       " 12.753659288604991,\n",
       " 9.852181099601001,\n",
       " 4.9011930299,\n",
       " 2.9304980099999995,\n",
       " 2.9600989999999996,\n",
       " 4.0001,\n",
       " -1.01,\n",
       " 1.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accumulate_discount(ep_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of chosen actions and weights sort of behaves like labels. This loss function is essentially just cross entropy loss (negative log likelihood loss), except the loss for each sample is weighted by the expected return at that timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.19327361, -1.47855409,  1.27444737, -0.41548711]),\n",
       " array([ 0.51988474,  1.54889377,  1.2939353 , -0.45033854]),\n",
       " array([ 0.11627388, -0.95530747, -0.72564121, -1.69475162]),\n",
       " array([ 1.98532673,  2.0308256 , -0.31792879,  0.97846826]),\n",
       " array([ 0.22967791,  1.3047228 , -0.30795531, -0.75400581])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_states = [np.random.randn(4) for i in range(step_batch_size)]\n",
    "batch_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3359, -0.3889],\n",
       "        [ 0.0339, -0.2518],\n",
       "        [ 0.2909, -0.2671],\n",
       "        [ 0.0165,  0.1521],\n",
       "        [ 0.3116, -0.0699]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations = net(torch.tensor(batch_states, dtype=torch.float32))\n",
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = np.random.randint(0, 2, size=(5,)).tolist()\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.8419900199999995, 3.8807979999999995, 5.9402, 3.98, 2.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cum_disc_rewards = accumulate_discount(np.random.randint(-2, 6, size=(5,)).tolist())\n",
    "cum_disc_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function using weird PyTorch objects eww."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-9163a07584ee>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_loss = loss_func(activations=torch.tensor(activations, dtype=torch.float32),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.9831)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_func(activations, chosen_acts, weights):\n",
    "    '''\n",
    "    outputs must be a tensor, shape is (~step_batch_size, n_acts), dtype is torch.float32\n",
    "    chosen_acts must be a tensor, shape is (~step_batch_size) dtype is torch.int32\n",
    "    weights must be a tensor, shape is (~step_batch_size) dtype is torch.float32\n",
    "    '''\n",
    "    assert activations.dtype == torch.float32\n",
    "    assert chosen_acts.dtype == torch.int32\n",
    "    assert weights.dtype == torch.float32\n",
    "    selected_log_probs = Categorical(logits=activations).log_prob(chosen_acts)  # Returns a batch of nll losses\n",
    "        # log_prob does cross entropy loss (softmax --> take prob corres to chosen class --> log --> negative)\n",
    "    return -(selected_log_probs * weights).mean()\n",
    "\n",
    "batch_loss = loss_func(activations=torch.tensor(activations, dtype=torch.float32), \n",
    "                       chosen_acts=torch.tensor(actions, dtype=torch.int32), \n",
    "                       weights=torch.tensor(cum_disc_rewards, dtype=torch.float32)\n",
    "                      )\n",
    "batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function manually yay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-5656c6242727>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_loss = loss_func_manual(activations=torch.tensor(activations, dtype=torch.float32),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.9831)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_func_manual(activations, chosen_acts, weights):\n",
    "    '''\n",
    "    outputs must be a tensor, shape is (~step_batch_size, n_acts), dtype is torch.float32\n",
    "    chosen_acts must be a tensor, shape is (~step_batch_size), dtype is torch.int64 because torch.gather is picky\n",
    "    weights must be a tensor, shape is (~step_batch_size), dtype is torch.float32\n",
    "    '''\n",
    "    assert activations.dtype == torch.float32\n",
    "    assert chosen_acts.dtype == torch.int64\n",
    "    assert weights.dtype == torch.float32\n",
    "    log_probs = nn.LogSoftmax(dim=-1)(activations) # Output has autoograd relationship to activations, for back propagation\n",
    "    selected_log_probs = torch.gather(log_probs, -1, chosen_acts.unsqueeze(1)).squeeze()\n",
    "    return -(selected_log_probs * weights).mean() # Negative expected return to trick optimizer into doing gradient ascent\n",
    "\n",
    "batch_loss = loss_func_manual(activations=torch.tensor(activations, dtype=torch.float32), \n",
    "                       chosen_acts=torch.tensor(actions, dtype=torch.int64), # torch.gather only works with int64\n",
    "                       weights=torch.tensor(cum_disc_rewards, dtype=torch.float32)\n",
    "                      )\n",
    "batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.LogSoftmax` returns a function object or whatever it's called. Its argument `dim` is the dimension over which to softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6670, -0.7200],\n",
       "         [-0.5605, -0.8462],\n",
       "         [-0.4526, -1.0106],\n",
       "         [-0.7632, -0.6277],\n",
       "         [-0.5205, -0.9020]], grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([0, 1, 1, 1, 0]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs = nn.LogSoftmax(dim=-1)(activations)\n",
    "chosen_acts = torch.randint(0,2,size=(5,), dtype=torch.int64)\n",
    "log_probs, chosen_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.gather` works like this. `log_probs` has shape (step_batch_size, n_acts) while chosen_acts has shape (step_batch_size). You need to put each of the indices in chosen_acts in its own array for `gather` to work, so use `unsqueeze` in the second dimension to make the shape (step_batch_size, n_acts). Finally, after gather, squeeze out the redundant second dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6670, -0.8462, -1.0106, -0.6277, -0.5205],\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_log_probs = torch.gather(log_probs, -1, chosen_acts.unsqueeze(1)).squeeze()\n",
    "selected_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8420, 3.8808, 5.9402, 3.9800, 2.0000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor(cum_disc_rewards, dtype=torch.float32)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8955, -3.2838, -6.0032, -2.4982, -1.0410], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_log_probs * weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test forward propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03926912,  2.31835182, -0.97210445, -0.22401287])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.random.randn(4)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the `nn.Sequential` object will take a single sample with no problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3876, 0.1584], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations = net(torch.tensor(state, dtype=torch.float32))\n",
    "activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test action sampling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Categorical` exhibits weird behaviour...\n",
    "\n",
    "`Categorical`'s `probs` argument takes in a tensor of 'probabilities' in range `[0, inf)` ie. non-negative but does not need to sum to 1, as the class will automatically normalize the values to make the distribution. Make sure to sigmoid or softmax activations before passing this argument.\n",
    "\n",
    "`Categorical`'s `logits` argument takes a tensor of values in range `(-inf, inf)` and will turn it into a probability distribution that sums to 1, probably with softmax but idk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize: tensor([0.1969, 0.1969, 0.1654, 0.4409])\n",
      "Then natural log: tensor([-1.6253, -1.6253, -1.7997, -0.8188])\n"
     ]
    }
   ],
   "source": [
    "probs_list = [0.25, 0.25, 0.21, 0.56]\n",
    "dist = torch.distributions.categorical.Categorical(probs=torch.tensor(probs_list))\n",
    "print(f\"Normalize: {dist.probs}\\nThen natural log: {dist.logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax of logits: tensor([0.0580, 0.1426, 0.2496, 0.5499])\n",
      "Then natural log: tensor([-2.8480, -1.9480, -1.3880, -0.5980])\n"
     ]
    }
   ],
   "source": [
    "logits_list = [-1.05, -0.15, 0.41, 1.20]\n",
    "dist = torch.distributions.categorical.Categorical(logits=torch.tensor(logits_list))\n",
    "print(f\"Softmax of logits: {dist.probs}\\nThen natural log: {dist.logits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name='CartPole-v0', \n",
    "          hidden_sizes=[32], \n",
    "          lr=1e-2, \n",
    "          num_epochs=50, \n",
    "          step_batch_size=5000, \n",
    "          render=False\n",
    "         ):\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    assert isinstance(env.observation_space, Box), \\\n",
    "        \"This example only works for envs with continuous state spaces.\"\n",
    "    assert isinstance(env.action_space, Discrete), \\\n",
    "        \"This example only works for envs with discrete action spaces.\"\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_acts = env.action_space.n\n",
    "    action_space = np.arange(n_acts, dtype=np.int64)\n",
    "    \n",
    "    net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts]).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        \n",
    "        # Epoch-specific variables, resets each epoch\n",
    "        batch_states = []      # State at each step, shape is (num steps over all episodes this epoch ie. >= step_batch_size, obs_dim)\n",
    "        batch_acts = []        # Action at each step, shape is (num steps over all episodes this epoch, n_acts)\n",
    "        batch_weights = []     # Cumulative future discounted reward at each step, shape is (num steps over all episodes this epoch)\n",
    "        batch_ep_rets = []     # Returns for each episode in epoch, shape is (num episodes this epoch)\n",
    "        batch_ep_lens = []     # Lengths (number of steps) of each episode in epoch, shape is (num episodes this epoch)\n",
    "        \n",
    "        # Episode-specific variables, resets each episode\n",
    "        cur_state = env.reset()\n",
    "        done = False\n",
    "        ep_rewards = []\n",
    "        render_episode = True\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            activations = net(torch.tensor(cur_state, dtype=torch.float32).to(device)).clone().detach()\n",
    "                # clone: make deep copy\n",
    "                # detach: separate from autograd computation graph,\n",
    "                    # to prevent accidentally changing gradient buffers with this calculation\n",
    "            action_probs = nn.Softmax(dim=-1)(activations).cpu().numpy()\n",
    "#             action = Categorical(probs=action_probs).sample().item()\n",
    "            action = np.random.choice(action_space, p=action_probs)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            batch_states.append(cur_state.copy())\n",
    "            batch_acts.append(action)\n",
    "            ep_rewards.append(reward)\n",
    "            \n",
    "            cur_state = next_state\n",
    "            \n",
    "            if render_episode and render:\n",
    "                env.render()\n",
    "            \n",
    "            if done:\n",
    "                # If episode over record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rewards), len(ep_rewards)\n",
    "                batch_ep_rets.append(ep_ret)\n",
    "                batch_ep_lens.append(ep_len)\n",
    "                \n",
    "                batch_weights.extend(accumulate_discount(ep_rewards, gamma=0.99))\n",
    "                \n",
    "                # Reset episode-specific variables\n",
    "                cur_state = env.reset()\n",
    "                done = False\n",
    "                ep_rewards = []        \n",
    "                render_episode = False\n",
    "                \n",
    "                if len(batch_states) >= step_batch_size:\n",
    "                    '''\n",
    "                    We are only allowed to break at the end of an episode.\n",
    "                    If at the end of this episode we finally have enough steps,\n",
    "                        then we take this opportunity to break and call it an epoch.\n",
    "                    '''\n",
    "                    break\n",
    "\n",
    "                    \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = loss_func_manual(activations=net(torch.tensor(batch_states, dtype=torch.float32).to(device)), \n",
    "                                      chosen_acts=torch.tensor(batch_acts, dtype=torch.int64).to(device), \n",
    "                                      weights=torch.tensor(batch_weights, dtype=torch.float32).to(device)\n",
    "                                     )\n",
    "        # Gradient flows from batch_loss -> activations -> parameters & hidden activations in network\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (epoch, batch_loss, np.mean(batch_ep_rets), np.mean(batch_ep_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 \t loss: 6.855 \t return: 17.106 \t ep_len: 17.106\n",
      "epoch:   2 \t loss: 7.233 \t return: 18.087 \t ep_len: 18.087\n",
      "epoch:   3 \t loss: 8.578 \t return: 20.946 \t ep_len: 20.946\n",
      "epoch:   4 \t loss: 8.806 \t return: 21.996 \t ep_len: 21.996\n",
      "epoch:   5 \t loss: 10.113 \t return: 24.995 \t ep_len: 24.995\n",
      "epoch:   6 \t loss: 11.703 \t return: 29.763 \t ep_len: 29.763\n",
      "epoch:   7 \t loss: 10.670 \t return: 28.764 \t ep_len: 28.764\n",
      "epoch:   8 \t loss: 12.359 \t return: 33.939 \t ep_len: 33.939\n",
      "epoch:   9 \t loss: 13.270 \t return: 37.722 \t ep_len: 37.722\n",
      "epoch:  10 \t loss: 14.625 \t return: 40.248 \t ep_len: 40.248\n",
      "epoch:  11 \t loss: 15.362 \t return: 44.009 \t ep_len: 44.009\n",
      "epoch:  12 \t loss: 15.018 \t return: 45.964 \t ep_len: 45.964\n",
      "epoch:  13 \t loss: 14.695 \t return: 45.090 \t ep_len: 45.090\n",
      "epoch:  14 \t loss: 16.569 \t return: 53.617 \t ep_len: 53.617\n",
      "epoch:  15 \t loss: 15.742 \t return: 51.429 \t ep_len: 51.429\n",
      "epoch:  16 \t loss: 17.410 \t return: 57.598 \t ep_len: 57.598\n",
      "epoch:  17 \t loss: 17.067 \t return: 59.388 \t ep_len: 59.388\n",
      "epoch:  18 \t loss: 15.662 \t return: 53.585 \t ep_len: 53.585\n",
      "epoch:  19 \t loss: 18.257 \t return: 66.697 \t ep_len: 66.697\n",
      "epoch:  20 \t loss: 17.825 \t return: 65.842 \t ep_len: 65.842\n",
      "epoch:  21 \t loss: 20.801 \t return: 78.281 \t ep_len: 78.281\n",
      "epoch:  22 \t loss: 20.160 \t return: 73.391 \t ep_len: 73.391\n",
      "epoch:  23 \t loss: 22.270 \t return: 89.750 \t ep_len: 89.750\n",
      "epoch:  24 \t loss: 24.070 \t return: 102.408 \t ep_len: 102.408\n",
      "epoch:  25 \t loss: 24.462 \t return: 103.592 \t ep_len: 103.592\n",
      "epoch:  26 \t loss: 25.150 \t return: 118.674 \t ep_len: 118.674\n",
      "epoch:  27 \t loss: 25.812 \t return: 128.075 \t ep_len: 128.075\n",
      "epoch:  28 \t loss: 25.795 \t return: 125.475 \t ep_len: 125.475\n",
      "epoch:  29 \t loss: 26.253 \t return: 133.316 \t ep_len: 133.316\n",
      "epoch:  30 \t loss: 27.347 \t return: 138.889 \t ep_len: 138.889\n",
      "epoch:  31 \t loss: 28.437 \t return: 156.970 \t ep_len: 156.970\n",
      "epoch:  32 \t loss: 27.644 \t return: 144.057 \t ep_len: 144.057\n",
      "epoch:  33 \t loss: 29.187 \t return: 165.000 \t ep_len: 165.000\n",
      "epoch:  34 \t loss: 27.956 \t return: 150.088 \t ep_len: 150.088\n",
      "epoch:  35 \t loss: 30.245 \t return: 182.107 \t ep_len: 182.107\n",
      "epoch:  36 \t loss: 30.226 \t return: 181.786 \t ep_len: 181.786\n",
      "epoch:  37 \t loss: 30.111 \t return: 179.357 \t ep_len: 179.357\n",
      "epoch:  38 \t loss: 30.168 \t return: 179.071 \t ep_len: 179.071\n",
      "epoch:  39 \t loss: 29.707 \t return: 177.793 \t ep_len: 177.793\n",
      "epoch:  40 \t loss: 30.107 \t return: 187.481 \t ep_len: 187.481\n",
      "epoch:  41 \t loss: 30.171 \t return: 193.462 \t ep_len: 193.462\n",
      "epoch:  42 \t loss: 30.577 \t return: 193.654 \t ep_len: 193.654\n",
      "epoch:  43 \t loss: 30.392 \t return: 190.111 \t ep_len: 190.111\n",
      "epoch:  44 \t loss: 30.332 \t return: 195.115 \t ep_len: 195.115\n",
      "epoch:  45 \t loss: 30.449 \t return: 197.154 \t ep_len: 197.154\n",
      "epoch:  46 \t loss: 30.191 \t return: 199.000 \t ep_len: 199.000\n",
      "epoch:  47 \t loss: 29.756 \t return: 198.385 \t ep_len: 198.385\n",
      "epoch:  48 \t loss: 29.718 \t return: 197.692 \t ep_len: 197.692\n",
      "epoch:  49 \t loss: 29.941 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  50 \t loss: 29.827 \t return: 199.577 \t ep_len: 199.577\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
