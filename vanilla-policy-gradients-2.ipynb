{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/conda/envs/fastai/lib/python3.8/site-packages (0.18.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from gym) (7.2.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/fastai/lib/python3.8/site-packages (from gym) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from gym) (1.19.4)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: future in /opt/conda/envs/fastai/lib/python3.8/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.7.0\n",
      "gym version: 0.18.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"gym version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, hidden_act_fn=nn.Tanh, output_act_fn=nn.Identity):\n",
    "    '''\n",
    "    sizes is list of integers specifying the number of nodes\n",
    "        in each layer of the network, including input and output layers.\n",
    "    Returns a torch.nn.Sequential object.\n",
    "    '''\n",
    "    assert isinstance(sizes, list)\n",
    "    layers = []\n",
    "    num_gaps = len(sizes) - 1\n",
    "    for i in range(num_gaps):\n",
    "        act_fn = hidden_act_fn if i < num_gaps-1 else output_act_fn\n",
    "        layers.extend([nn.Linear(sizes[i], sizes[i+1]), act_fn()])\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (3): Identity()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = mlp([4,32,2])\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Future Discounted Rewards helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_discount(trajec_rewards, gamma=0.99):\n",
    "    '''\n",
    "    trajec_rewards must be a list of scalar reward values for each step.\n",
    "    Returns a list of reverse-accumlated, discounted rewards, where each value\n",
    "        represents the cumulative discounted rewards from that step onwards up to the end of the trajectory.\n",
    "    '''\n",
    "    assert isinstance(trajec_rewards, list)\n",
    "    trajec_len = len(trajec_rewards)\n",
    "    cum_disc_rewards = [None for i in range(trajec_len)]\n",
    "    for step in reversed(range(trajec_len)):\n",
    "        cum_disc_rewards[step] = trajec_rewards[step] + gamma * (cum_disc_rewards[step+1] if step+1 < trajec_len else 0)\n",
    "    return cum_disc_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 0, -1, -1, 2, 0, -2, -1, 3, 5, 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_rewards = np.random.randint(-2, 6, size=(12,)).tolist()\n",
    "ep_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13.312010071311759,\n",
       " 8.395969769001777,\n",
       " 5.45047451414321,\n",
       " 5.505529812265868,\n",
       " 6.571242234611988,\n",
       " 7.647719428900999,\n",
       " 5.704767099899999,\n",
       " 5.762391009999999,\n",
       " 7.840798999999999,\n",
       " 8.9301,\n",
       " 5.99,\n",
       " 1.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accumulate_discount(ep_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of chosen actions and weights sort of behaves like labels. This loss function is essentially just cross entropy loss (negative log likelihood loss), except the loss for each sample is weighted by the expected return at that timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(activations, chosen_acts, weights):\n",
    "    '''\n",
    "    outputs must be a tensor, shape is (~step_batch_size, n_acts), dtype is torch.float32\n",
    "    chosen_acts must be a tensor, shape is (~step_batch_size) dtype is torch.int32\n",
    "    weights must be a tensor, shape is (~step_batch_size) dtype is torch.float32\n",
    "    '''\n",
    "    assert activations.dtype == torch.float32\n",
    "    assert chosen_acts.dtype == torch.int32\n",
    "    assert weights.dtype == torch.float32\n",
    "    log_probs = Categorical(logits=activations).log_prob(chosen_acts)  # Returns a batch of nll losses\n",
    "        # log_prob does cross entropy loss (softmax --> take prob corres to chosen class --> log --> negative)\n",
    "    return -(log_probs * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2520, -1.0268,  0.7331, -0.7374],\n",
       "        [ 2.5440, -0.7746,  0.5921,  0.2919],\n",
       "        [-0.9547, -0.1100,  1.3107, -2.3316],\n",
       "        [-1.5341, -0.4752, -0.3159, -1.7549],\n",
       "        [-1.1733,  0.8263, -0.3815,  0.4881]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.randn(5, 4, dtype=torch.float32)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2991,  0.0791],\n",
       "        [-0.5278, -0.1625],\n",
       "        [ 0.5637,  0.3496],\n",
       "        [ 0.8434,  0.3493],\n",
       "        [ 0.2714,  0.3622]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations = net(inputs)\n",
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = np.random.randint(0, 2, size=(5,))\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.98010298, -0.02009800000000017, 0.9897999999999998, 3.02, -2.0]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cum_disc_rewards = accumulate_discount(np.random.randint(-2, 6, size=(5,)).tolist())\n",
    "cum_disc_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-78-d8dcc83c8864>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_loss = loss_func(activations=torch.tensor(activations, dtype=torch.float32),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7928)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss = loss_func(activations=torch.tensor(activations, dtype=torch.float32), \n",
    "                       chosen_acts=torch.tensor(actions, dtype=torch.int32), \n",
    "                       weights=torch.tensor(cum_disc_rewards, dtype=torch.float32)\n",
    "                      )\n",
    "batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test forward propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.30437415, 0.7246936 , 1.55096719, 0.12279306])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.random.randn(4)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the `nn.Sequential` object will take a single sample with no problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6728,  0.1078], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations = net(torch.tensor(state, dtype=torch.float32))\n",
    "activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test action sampling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Categorical` exhibits weird behaviour...\n",
    "\n",
    "`Categorical`'s `probs` argument takes in a tensor of 'probabilities' in range `[0, inf)` ie. non-negative but does not need to sum to 1, as the class will automatically normalize the values to make the distribution. Make sure to sigmoid or softmax activations before passing this argument.\n",
    "\n",
    "`Categorical`'s `logits` argument takes a tensor of values in range `(-inf, inf)` and will turn it into a probability distribution that sums to 1, probably with softmax but idk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize: tensor([0.1969, 0.1969, 0.1654, 0.4409])\n",
      "Then natural log: tensor([-1.6253, -1.6253, -1.7997, -0.8188])\n"
     ]
    }
   ],
   "source": [
    "probs_list = [0.25, 0.25, 0.21, 0.56]\n",
    "dist = torch.distributions.categorical.Categorical(probs=torch.tensor(probs_list))\n",
    "print(f\"Normalize: {dist.probs}\\nThen natural log: {dist.logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax of logits: tensor([0.0580, 0.1426, 0.2496, 0.5499])\n",
      "Then natural log: tensor([-2.8480, -1.9480, -1.3880, -0.5980])\n"
     ]
    }
   ],
   "source": [
    "logits_list = [-1.05, -0.15, 0.41, 1.20]\n",
    "dist = torch.distributions.categorical.Categorical(logits=torch.tensor(logits_list))\n",
    "print(f\"Softmax of logits: {dist.probs}\\nThen natural log: {dist.logits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name='CartPole-v0', \n",
    "          hidden_sizes=[32], \n",
    "          lr=1e-2, \n",
    "          num_epochs=50, \n",
    "          step_batch_size=5000, \n",
    "          render=False\n",
    "         ):\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    assert isinstance(env.observation_space, Box), \\\n",
    "        \"This example only works for envs with continuous state spaces.\"\n",
    "    assert isinstance(env.action_space, Discrete), \\\n",
    "        \"This example only works for envs with discrete action spaces.\"\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_acts = env.action_space.n\n",
    "    \n",
    "    net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        \n",
    "        # Epoch-specific variables, resets each epoch\n",
    "        batch_states = []      # State at each step, shape is (num steps over all episodes this epoch ie. >= step_batch_size, obs_dim)\n",
    "        batch_acts = []        # Action at each step, shape is (num steps over all episodes this epoch, n_acts)\n",
    "        batch_weights = []     # Cumulative future discounted reward at each step, shape is (num steps over all episodes this epoch)\n",
    "        batch_ep_rets = []     # Returns for each episode in epoch, shape is (num episodes this epoch)\n",
    "        batch_ep_lens = []     # Lengths (number of steps) of each episode in epoch, shape is (num episodes this epoch)\n",
    "        \n",
    "        # Episode-specific variables, resets each episode\n",
    "        cur_state = env.reset()\n",
    "        done = False\n",
    "        ep_rewards = []\n",
    "        render_episode = True\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            activations = net(torch.tensor(cur_state, dtype=torch.float32))\n",
    "            action = Categorical(logits=activations).sample().item()\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            batch_states.append(cur_state.copy())\n",
    "            batch_acts.append(action)\n",
    "            ep_rewards.append(reward)\n",
    "            \n",
    "            cur_state = next_state\n",
    "            \n",
    "            if render_episode and render:\n",
    "                env.render()\n",
    "            \n",
    "            if done:\n",
    "                # If episode over record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rewards), len(ep_rewards)\n",
    "                batch_ep_rets.append(ep_ret)\n",
    "                batch_ep_lens.append(ep_len)\n",
    "                \n",
    "                batch_weights.extend(accumulate_discount(ep_rewards))\n",
    "                \n",
    "                # Reset episode-specific variables\n",
    "                cur_state = env.reset()\n",
    "                done = False\n",
    "                ep_rewards = []        \n",
    "                render_episode = False\n",
    "                \n",
    "                if len(batch_states) >= step_batch_size:\n",
    "                    '''\n",
    "                    We are only allowed to break at the end of an episode.\n",
    "                    If at the end of this episode we finally have enough steps,\n",
    "                        then we take this opportunity to break and call it an epoch.\n",
    "                    '''\n",
    "                    break\n",
    "\n",
    "                    \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = loss_func(activations=net(torch.tensor(batch_states, dtype=torch.float32)), \n",
    "                               chosen_acts=torch.tensor(batch_acts, dtype=torch.int32), \n",
    "                               weights=torch.tensor(batch_weights, dtype=torch.float32)\n",
    "                              )\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (epoch, batch_loss, np.mean(batch_ep_rets), np.mean(batch_ep_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9999 \t loss: 9.801 \t return: 23.986 \t ep_len: 23.986\n",
      "epoch: 9999 \t loss: 9.919 \t return: 24.544 \t ep_len: 24.544\n",
      "epoch: 9999 \t loss: 11.491 \t return: 28.600 \t ep_len: 28.600\n",
      "epoch: 9999 \t loss: 11.989 \t return: 30.303 \t ep_len: 30.303\n",
      "epoch: 9999 \t loss: 13.136 \t return: 33.079 \t ep_len: 33.079\n",
      "epoch: 9999 \t loss: 13.969 \t return: 40.008 \t ep_len: 40.008\n",
      "epoch: 9999 \t loss: 15.235 \t return: 43.241 \t ep_len: 43.241\n",
      "epoch: 9999 \t loss: 15.558 \t return: 47.781 \t ep_len: 47.781\n",
      "epoch: 9999 \t loss: 16.142 \t return: 50.520 \t ep_len: 50.520\n",
      "epoch: 9999 \t loss: 16.218 \t return: 52.905 \t ep_len: 52.905\n",
      "epoch: 9999 \t loss: 16.700 \t return: 54.543 \t ep_len: 54.543\n",
      "epoch: 9999 \t loss: 18.658 \t return: 63.087 \t ep_len: 63.087\n",
      "epoch: 9999 \t loss: 19.156 \t return: 69.162 \t ep_len: 69.162\n",
      "epoch: 9999 \t loss: 19.559 \t return: 74.044 \t ep_len: 74.044\n",
      "epoch: 9999 \t loss: 20.407 \t return: 79.921 \t ep_len: 79.921\n",
      "epoch: 9999 \t loss: 21.241 \t return: 82.426 \t ep_len: 82.426\n",
      "epoch: 9999 \t loss: 21.964 \t return: 88.509 \t ep_len: 88.509\n",
      "epoch: 9999 \t loss: 22.984 \t return: 100.235 \t ep_len: 100.235\n",
      "epoch: 9999 \t loss: 23.265 \t return: 101.200 \t ep_len: 101.200\n",
      "epoch: 9999 \t loss: 24.196 \t return: 113.795 \t ep_len: 113.795\n",
      "epoch: 9999 \t loss: 25.213 \t return: 125.725 \t ep_len: 125.725\n",
      "epoch: 9999 \t loss: 24.877 \t return: 120.643 \t ep_len: 120.643\n",
      "epoch: 9999 \t loss: 24.368 \t return: 118.837 \t ep_len: 118.837\n",
      "epoch: 9999 \t loss: 24.697 \t return: 120.452 \t ep_len: 120.452\n",
      "epoch: 9999 \t loss: 25.845 \t return: 129.282 \t ep_len: 129.282\n",
      "epoch: 9999 \t loss: 25.942 \t return: 135.184 \t ep_len: 135.184\n",
      "epoch: 9999 \t loss: 27.084 \t return: 147.343 \t ep_len: 147.343\n",
      "epoch: 9999 \t loss: 28.095 \t return: 156.312 \t ep_len: 156.312\n",
      "epoch: 9999 \t loss: 27.191 \t return: 149.853 \t ep_len: 149.853\n",
      "epoch: 9999 \t loss: 27.805 \t return: 161.812 \t ep_len: 161.812\n",
      "epoch: 9999 \t loss: 28.051 \t return: 157.500 \t ep_len: 157.500\n",
      "epoch: 9999 \t loss: 27.112 \t return: 151.545 \t ep_len: 151.545\n",
      "epoch: 9999 \t loss: 28.452 \t return: 164.065 \t ep_len: 164.065\n",
      "epoch: 9999 \t loss: 27.940 \t return: 168.667 \t ep_len: 168.667\n",
      "epoch: 9999 \t loss: 29.258 \t return: 181.429 \t ep_len: 181.429\n",
      "epoch: 9999 \t loss: 29.296 \t return: 185.679 \t ep_len: 185.679\n",
      "epoch: 9999 \t loss: 29.443 \t return: 189.296 \t ep_len: 189.296\n",
      "epoch: 9999 \t loss: 30.375 \t return: 197.462 \t ep_len: 197.462\n",
      "epoch: 9999 \t loss: 29.293 \t return: 186.667 \t ep_len: 186.667\n",
      "epoch: 9999 \t loss: 29.980 \t return: 195.192 \t ep_len: 195.192\n",
      "epoch: 9999 \t loss: 30.292 \t return: 196.962 \t ep_len: 196.962\n",
      "epoch: 9999 \t loss: 29.531 \t return: 197.654 \t ep_len: 197.654\n",
      "epoch: 9999 \t loss: 29.863 \t return: 192.769 \t ep_len: 192.769\n",
      "epoch: 9999 \t loss: 29.447 \t return: 194.885 \t ep_len: 194.885\n",
      "epoch: 9999 \t loss: 29.478 \t return: 198.231 \t ep_len: 198.231\n",
      "epoch: 9999 \t loss: 29.575 \t return: 196.000 \t ep_len: 196.000\n",
      "epoch: 9999 \t loss: 29.015 \t return: 187.444 \t ep_len: 187.444\n",
      "epoch: 9999 \t loss: 28.745 \t return: 185.630 \t ep_len: 185.630\n",
      "epoch: 9999 \t loss: 28.229 \t return: 178.607 \t ep_len: 178.607\n",
      "epoch: 9999 \t loss: 28.046 \t return: 179.429 \t ep_len: 179.429\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
